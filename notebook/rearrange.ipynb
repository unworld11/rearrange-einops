{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmDYFItyw1tK"
      },
      "source": [
        "# Implement rearrange function from einops library from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aigP_ZFzwz-Z"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o4MKKCQ_VBC"
      },
      "source": [
        "Loading Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFpLIagt_PdZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from math import prod\n",
        "import json\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7hYOp61_XYy"
      },
      "source": [
        "Some initial utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql4KHSVK_Wvg"
      },
      "outputs": [],
      "source": [
        "def _tokenize(pattern_str):\n",
        "    \"\"\"\n",
        "    Tokenize a pattern string into a list of items:\n",
        "    - '...' (ellipsis)\n",
        "    - '(...)' (group)\n",
        "    - '\\w+' (single axis, e.g. 'b', 'h', 'h1')\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = re.findall(r'\\.\\.\\.|\\([\\w\\s\\-]+\\)|-?\\w+', pattern_str)\n",
        "    return tokens\n",
        "\n",
        "def to_numpy_array(input_data):\n",
        "    \"\"\"\n",
        "    Converts the input data to a NumPy array.\n",
        "    Supports:\n",
        "        - list\n",
        "        - NumPy array (returned as-is)\n",
        "        - PyTorch tensor (converted to NumPy array)\n",
        "    Raises:\n",
        "        - TypeError if input data is of an unsupported type.\n",
        "    \"\"\"\n",
        "    if isinstance(input_data, np.ndarray):\n",
        "        return input_data\n",
        "    elif isinstance(input_data, list):\n",
        "        return np.array(input_data)\n",
        "    elif isinstance(input_data, torch.Tensor):\n",
        "        return input_data.numpy()\n",
        "    else:\n",
        "        raise TypeError(f\"Unsupported input type: {type(input_data)}. Expected list, NumPy array, or PyTorch tensor.\")\n",
        "\n",
        "def clean_singletons_in_parentheses(pattern):\n",
        "    \"\"\"\n",
        "    Cleans up singletons in parentheses in einops rearrange patterns.\n",
        "\n",
        "    Parameters:\n",
        "        pattern (str): The rearrange pattern to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned pattern.\n",
        "\n",
        "    For example,\n",
        "    Input: b (c 1 1) h w -> b c h w\n",
        "    Output:  b c h w -> b c h w\n",
        "    \"\"\"\n",
        "\n",
        "    # Define a regex to match parentheses containing dimensions\n",
        "    paren_regex = re.compile(r'\\(([^()]+)\\)')\n",
        "\n",
        "    def replace_function(match):\n",
        "        # Extract inside of parentheses\n",
        "        content = match.group(1)\n",
        "        # Remove `1` from the dimensions\n",
        "        cleaned_content = ' '.join([dim for dim in content.split() if dim != '1'])\n",
        "        # If only one dimension remains, remove parentheses\n",
        "        return cleaned_content if ' ' not in cleaned_content else f'({cleaned_content})'\n",
        "\n",
        "    # Replace all parentheses content using the defined function\n",
        "    cleaned_pattern = paren_regex.sub(replace_function, pattern)\n",
        "\n",
        "    # Remove redundant spaces and ensure clean formatting\n",
        "    cleaned_pattern = re.sub(r'\\s+', ' ', cleaned_pattern).strip()\n",
        "\n",
        "    return cleaned_pattern\n",
        "\n",
        "def unexpected_chars_checker(s):\n",
        "    \"\"\"\n",
        "    Check for unexpected characters in the pattern.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if no unexpected characters are found, raises a ValueError otherwise.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If unexpected characters are found in the pattern.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the valid pattern for allowed tokens\n",
        "    valid_pattern = re.compile(r'^([a-zA-Z]+[1-9][0-9]*|[a-zA-Z]+|\\.\\.\\.|1)$')  # Matches h1, h11, a, b, ..., or 1\n",
        "\n",
        "    # Remove '->' and parentheses for simpler parsing\n",
        "    tokens = s.replace('->', '').translate(str.maketrans('', '', '()')).split()\n",
        "\n",
        "    # Check each token against the valid pattern\n",
        "    unexpected_chars = [token for token in tokens if not valid_pattern.match(token)]\n",
        "\n",
        "    if unexpected_chars:\n",
        "        raise ValueError(f\"Unexpected characters found in pattern: {unexpected_chars}\")\n",
        "\n",
        "    # return True  # No unexpected characters found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI1lGU5mwg9g"
      },
      "source": [
        "### Pattern Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCeMomec_ekl"
      },
      "outputs": [],
      "source": [
        "class Validator:\n",
        "    def __init__(self, array, pattern, **kwargs):\n",
        "        self.array = to_numpy_array(array)\n",
        "        self._is_empty_array()\n",
        "        unexpected_chars_checker(pattern)\n",
        "        self.pattern = clean_singletons_in_parentheses(pattern)\n",
        "        self.kwargs = kwargs\n",
        "        self.array_shape = self.array.shape\n",
        "\n",
        "        self.input_str, self.output_str = self._parse_pattern()\n",
        "        self.input_tokens = _tokenize(self.input_str)\n",
        "        self.output_tokens = _tokenize(self.output_str)\n",
        "\n",
        "    def _is_empty_array(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Checks if the given NumPy array is empty.\n",
        "\n",
        "        Args:\n",
        "            array (numpy.ndarray): The input array to check.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the array is empty.\n",
        "        \"\"\"\n",
        "        if self.array.size == 0:\n",
        "            raise ValueError(\"The input NumPy array is empty. Please provide a valid array.\")\n",
        "\n",
        "    def _parse_pattern(self):\n",
        "        \"\"\"\n",
        "        Parses the pattern and splits it into input and output parts.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the input and output parts of the pattern.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            input_str, output_str = self.pattern.split('->')\n",
        "            return input_str.strip(), output_str.strip()\n",
        "\n",
        "        except ValueError:\n",
        "            raise ValueError(\"Pattern must be in the form 'input -> output'\")\n",
        "\n",
        "    def stripped_order(self, d):\n",
        "        \"\"\"\n",
        "        Removes all paranthesis from a list and returns a list\n",
        "\n",
        "        [\"a\", \"(b c)\"] -> [\"a\", \"b\", \"c\"]\n",
        "        \"\"\"\n",
        "\n",
        "        return \" \".join(d).replace(\"(\", \"\").replace(\")\", \"\").split(\" \")\n",
        "\n",
        "    def ellipsis_checker(self):\n",
        "        \"\"\"\n",
        "        Checks if more than ellipsis is in input or output pattern.\n",
        "        Raises ValueError if found.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.input_tokens.count('...') > 1 or self.output_tokens.count('...') > 1:\n",
        "            raise ValueError(\"Expression may contain dots only inside ellipsis (...); only one ellipsis for tensor\")\n",
        "\n",
        "    def identifier_match_checker(self):\n",
        "        \"\"\"\n",
        "        Checks for discrepancies between identifiers in the input and output pattern.\n",
        "        Raises ValeError if there are differences in identifiers or if some identifiers are duplicated.\n",
        "        \"\"\"\n",
        "\n",
        "        input_tokens_stripped = self.stripped_order(self.input_tokens)\n",
        "        output_tokens_stripped = self.stripped_order(self.output_tokens)\n",
        "\n",
        "        input_tokens_stripped_filtered = [item for item in input_tokens_stripped if item != '1']\n",
        "        output_tokens_stripped_filtered = [item for item in output_tokens_stripped if item != '1']\n",
        "\n",
        "        if len(set(input_tokens_stripped_filtered)) != len(input_tokens_stripped_filtered):\n",
        "            raise ValueError(f\"Input pattern {self.input_str} contains duplicate dimension\")\n",
        "\n",
        "        if len(set(output_tokens_stripped_filtered)) != len(output_tokens_stripped_filtered):\n",
        "            raise ValueError(f\"Input pattern {self.output_str} contains duplicate dimension\")\n",
        "\n",
        "        missing_in_output = set(input_tokens_stripped_filtered) - set(output_tokens_stripped_filtered)\n",
        "        extra_in_output = set(output_tokens_stripped_filtered) - set(input_tokens_stripped_filtered)\n",
        "\n",
        "        if missing_in_output:\n",
        "            raise ValueError(f\"Identifiers only on one side of expression (should be on both): {missing_in_output}\")\n",
        "        if extra_in_output:\n",
        "            raise ValueError(f\"Identifiers only on one side of expression (should be on both): {extra_in_output}\")\n",
        "\n",
        "    def input_token_mapper(self):\n",
        "        \"\"\"\n",
        "        Map the input numpy array shape index positions to the input part of the pattern string.\n",
        "\n",
        "        Input -\n",
        "            array = np.random.randn(2, 3, 4)\n",
        "            input_tokens = [\"a\", \"b\", \"c\"]\n",
        "        Output -\n",
        "            input_tokens_mapping = {\n",
        "                'a': 0,\n",
        "                'b': 1,\n",
        "                'c': 2\n",
        "            }\n",
        "            input_tokens_shape_mapping = {\n",
        "                'a': 2,\n",
        "                'b': 3,\n",
        "                'c': 4\n",
        "            }\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        ellipsis_count = self.input_tokens.count('...')\n",
        "        non_ellipsis_tokens = [tok for tok in self.input_tokens if tok != '...']\n",
        "\n",
        "        if ellipsis_count > 1:\n",
        "            raise ValueError(\"Pattern can have at most one ellipsis ('...').\")\n",
        "\n",
        "        # Check if token count matches array dimensions (unless ellipsis exists)\n",
        "        if ellipsis_count == 0 and len(self.input_tokens) != len(self.array_shape):\n",
        "            raise ValueError(\n",
        "                f\"Number of input tokens ({len(self.input_tokens)}) must match the array dimensions ({len(self.array_shape)}) unless using ellipsis ('...').\"\n",
        "            )\n",
        "        if ellipsis_count == 1 and len(non_ellipsis_tokens) > len(self.array_shape):\n",
        "            raise ValueError(\n",
        "                f\"Pattern with ellipsis ('...') must not have more explicit tokens ({len(non_ellipsis_tokens)}) than array dimensions ({len(self.array_shape)}).\"\n",
        "            )\n",
        "\n",
        "       # Assign index positions to tokens\n",
        "        input_tokens_mapping = {}\n",
        "        array_shape_indices = list(range(len(self.array_shape)))  # [0, 1, 2, ..., len(shape)-1]\n",
        "        input_tokens_shape_mapping = {}\n",
        "\n",
        "        if '...' in self.input_tokens:\n",
        "            # Handle ellipsis (map it to remaining index positions)\n",
        "            ellipsis_index = self.input_tokens.index('...')\n",
        "            ellipsis_dims = len(array_shape_indices) - len(non_ellipsis_tokens)\n",
        "            if ellipsis_dims < 0:\n",
        "                raise ValueError(\n",
        "                    f\"Ellipsis ('...') is invalid: not enough array dimensions to map remaining tokens.\"\n",
        "                )\n",
        "            ellipsis_mapping = array_shape_indices[:ellipsis_dims]\n",
        "            input_tokens_mapping['...'] = ellipsis_mapping\n",
        "            array_shape_indices = array_shape_indices[ellipsis_dims:]  # Remove mapped indices\n",
        "            input_tokens_shape_mapping['...'] = ellipsis_mapping\n",
        "\n",
        "        # Validate and map remaining tokens to index positions\n",
        "\n",
        "        singleton_count = 0\n",
        "\n",
        "        for token, index in zip(non_ellipsis_tokens, array_shape_indices):\n",
        "            if token == '1':\n",
        "                # Validate that the corresponding dimension is 1\n",
        "                if self.array_shape[index] != 1:\n",
        "                    raise ValueError(\n",
        "                        f\"Dimension for token '1' must be 1, but got {self.array_shape[index]} at index {index}.\"\n",
        "                    )\n",
        "                singleton_count += 1\n",
        "                input_tokens_mapping[\"singleton_\"+ str(singleton_count)] = index\n",
        "                input_tokens_shape_mapping[\"singleton_\"+ str(singleton_count)] = self.array_shape[index]\n",
        "            else:\n",
        "                input_tokens_mapping[token] = index\n",
        "                input_tokens_shape_mapping[token] = self.array_shape[index]\n",
        "\n",
        "        self.input_tokens_mapping = input_tokens_mapping\n",
        "        self.input_tokens_shape_mapping = input_tokens_shape_mapping\n",
        "        # print(\"Input Token Mapping: \", self.input_tokens_mapping)\n",
        "        # print(\"Input Token Shape Mapping: \", self.input_tokens_shape_mapping)\n",
        "\n",
        "    def output_tokens_mapper(self):\n",
        "        \"\"\"\n",
        "        Create mapping from output part of the pattern string.\n",
        "\n",
        "        Input -\n",
        "            output_tokens = [\"a\", \"b\", \"c\"]\n",
        "        Output -\n",
        "            output_tokens_mapping = {\n",
        "                'a': 0,\n",
        "                'b': 1,\n",
        "                'c': 2\n",
        "            }\n",
        "        \"\"\"\n",
        "\n",
        "        _, output_str = self.pattern.split('->')\n",
        "        self.output_str = output_str.strip()\n",
        "        self.output_tokens = _tokenize(output_str)\n",
        "\n",
        "        # Map output tokens to index positions\n",
        "        output_tokens_mapping = {}\n",
        "        current_index = 0\n",
        "\n",
        "        singleton_count = 0\n",
        "        for token in self.output_tokens:\n",
        "            # if token == '...':\n",
        "            #     output_tokens_mapping[token] = current_index\n",
        "            #     current_index += 1\n",
        "            if token == '1':\n",
        "                singleton_count += 1\n",
        "                # Fixed dimension (explicitly adds dimension of size 1)\n",
        "                output_tokens_mapping[\"singleton_\"+str(singleton_count)] = current_index\n",
        "                current_index += 1\n",
        "            else:\n",
        "                # Map single axis\n",
        "                output_tokens_mapping[token] = current_index\n",
        "                current_index += 1\n",
        "\n",
        "        self.output_tokens_mapping = output_tokens_mapping\n",
        "        # print(\"Output Token Mapping: \", self.output_tokens_mapping)\n",
        "\n",
        "    def empty_ellipsis_checker(self):\n",
        "        \"\"\"\n",
        "        Ellipsis in the input can be mapped to an empty list i.e. there are no axes that it points to.\n",
        "        In cases like this, ellipsis becomes unnecessary to handle. So, this function removes them from the mappings and updates the pattern\n",
        "        \"\"\"\n",
        "\n",
        "        if('...') in list(self.input_tokens_mapping.keys()) and len(self.input_tokens_mapping['...']) == 0:\n",
        "            del self.input_tokens_mapping['...']\n",
        "            del self.input_tokens_shape_mapping['...']\n",
        "            new_pattern = self.pattern.replace('...', '')\n",
        "\n",
        "            self.pattern = new_pattern.lstrip().rstrip()\n",
        "\n",
        "    def validate_and_return(self):\n",
        "        \"\"\"\n",
        "        Validates the pattern, tokens, and array shape, then returns mappings.\n",
        "        \"\"\"\n",
        "\n",
        "        self.ellipsis_checker()\n",
        "        self.identifier_match_checker()\n",
        "        self.input_token_mapper()\n",
        "        self.empty_ellipsis_checker()\n",
        "        self.output_tokens_mapper()\n",
        "\n",
        "        return (\n",
        "            self.array,\n",
        "            self.input_tokens_mapping,\n",
        "            self.input_tokens_shape_mapping,\n",
        "            self.output_tokens_mapping,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJwrFWKJAKWQ"
      },
      "source": [
        "Utility functions after creating mappings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iP6BAE0__yJH"
      },
      "outputs": [],
      "source": [
        "def tokens_from_paranthesis(input_set):\n",
        "    \"\"\"\n",
        "    Get identifiers from paranthesis\n",
        "    \"\"\"\n",
        "\n",
        "    paranth_set = set()\n",
        "    for item in input_set:\n",
        "        # Check if the item contains parentheses\n",
        "        if '(' in item and ')' in item:\n",
        "            # Extract elements inside parentheses and split by whitespace\n",
        "            inner_elements = re.findall(r'\\((.*?)\\)', item)\n",
        "            for group in inner_elements:\n",
        "                paranth_set.update(group.split())\n",
        "\n",
        "    return paranth_set\n",
        "\n",
        "def check_extra_arguments(input_mapping, **kwargs):\n",
        "    \"\"\"\n",
        "    Validates that no extra arguments are provided that are not part of the input mapping.\n",
        "\n",
        "    Args:\n",
        "        input_mapping (dict): Input token mapping.\n",
        "        kwargs: Additional arguments provided to the function.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If extra arguments are detected.\n",
        "    \"\"\"\n",
        "    allowed_tokens = tokens_from_paranthesis(set(input_mapping.keys()))\n",
        "    provided_tokens = set(kwargs.keys())\n",
        "\n",
        "    # Find extra arguments\n",
        "    extra_tokens = provided_tokens - allowed_tokens\n",
        "\n",
        "    if extra_tokens:\n",
        "        raise ValueError(f\"Extra arguments provided: {extra_tokens}. \"\n",
        "                         f\"Allowed arguments are: {list(allowed_tokens)}.\")\n",
        "\n",
        "    # print(\"No extra and unnecessary arguments provided.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ct0jSUnAO8a"
      },
      "outputs": [],
      "source": [
        "def get_additional_args(input_mapping, shape_mapping, **kwargs):\n",
        "    \"\"\"\n",
        "    Validates parentheses in input token mappings and ensures the arguments\n",
        "    provided match the required shapes in the shape mapping. Returns all\n",
        "    arguments, including inferred ones.\n",
        "\n",
        "    Args:\n",
        "        input_mapping (dict): Input token mapping.\n",
        "        shape_mapping (dict): Input shape mapping.\n",
        "        kwargs: Additional arguments corresponding to tokens inside parentheses.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of all arguments (inferred + original).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If parentheses validation fails.\n",
        "    \"\"\"\n",
        "    inferred_args = {}\n",
        "\n",
        "    for token, index in input_mapping.items():\n",
        "        if '(' in token and ')' in token:\n",
        "            # Extract tokens inside parentheses\n",
        "            inner_tokens = token.strip('()').split()\n",
        "\n",
        "            # Retrieve the shape value for the grouped token\n",
        "            expected_shape = shape_mapping[token]\n",
        "\n",
        "            # Check provided arguments\n",
        "            provided_args = [kwargs[arg] for arg in inner_tokens if arg in kwargs]\n",
        "\n",
        "            if len(provided_args) < len(inner_tokens):\n",
        "                # Try to infer the other argument\n",
        "                product = 1\n",
        "                for arg_value in provided_args:\n",
        "                    product *= arg_value\n",
        "                if expected_shape % product != 0:\n",
        "                    raise ValueError(f\"Could not infer sizes for {set(inner_tokens) - set(list(kwargs.keys()))}.\")\n",
        "                else:\n",
        "                    inferred_value = expected_shape // product\n",
        "                    inferred_token = [t for t in inner_tokens if t not in kwargs][0]\n",
        "                    inferred_args[inferred_token] = inferred_value\n",
        "                    # print(f\"Inferred value for {inferred_token}: {inferred_value}\")\n",
        "\n",
        "            elif len(provided_args) == len(inner_tokens):\n",
        "                # Multiple arguments must exactly match the shape value when multiplied\n",
        "                product = 1\n",
        "                for arg_value in provided_args:\n",
        "                    product *= arg_value\n",
        "                if product != expected_shape:\n",
        "                    raise ValueError(f\"Product of arguments {inner_tokens} ({provided_args}) does not match \"\n",
        "                                     f\"the expected shape {expected_shape} for token {token}.\")\n",
        "            else:\n",
        "                # No arguments provided, cannot validate\n",
        "                raise ValueError(f\"Missing required arguments for token: {token}. \"\n",
        "                                 f\"Expected at least one of {inner_tokens}.\")\n",
        "\n",
        "    # Combine original and inferred arguments\n",
        "    all_args = {**kwargs, **inferred_args}\n",
        "    # print(\"All parentheses checks passed.\")\n",
        "    return all_args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7xdhjegAi4_"
      },
      "source": [
        "### Input Pattern based Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSuJZUh-Ak4I"
      },
      "outputs": [],
      "source": [
        "def input_based_transformation(array, input_mapping, input_shape_mapping, **kwargs):\n",
        "    \"\"\"\n",
        "    Transforms the input array based on input mapping, ellipsis (`...`), and additional parameters for parentheses expansions.\n",
        "\n",
        "    Parameters:\n",
        "    - array: np.ndarray, the input array to transform.\n",
        "    - pattern: str, the input pattern.\n",
        "    - input_mapping: dict, maps each dimension name in the pattern to its index in the array.\n",
        "    - input_shape_mapping: dict, maps each dimension name in the pattern to its size in the array.\n",
        "    - kwargs: additional arguments for dimensions inside parentheses, e.g., h1, h, w1, w.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: transformed array.\n",
        "    \"\"\"\n",
        "    original_shape = list(array.shape)\n",
        "    new_shape = []\n",
        "\n",
        "    for key, index in input_mapping.items():\n",
        "        if key == '...':  # Handle ellipsis\n",
        "            ellipsis_dims = [original_shape[i] for i in index]\n",
        "            new_shape.extend(ellipsis_dims)\n",
        "        elif '(' in key and ')' in key:  # Handle parentheses\n",
        "            # Get the components inside the parentheses\n",
        "            components = key.strip('()').split()\n",
        "\n",
        "            # Ensure the components exist in the kwargs\n",
        "            if not all(dim in kwargs for dim in components):\n",
        "                raise ValueError(f\"Missing dimensions {components} for expanding '{key}'.\")\n",
        "\n",
        "            # Replace the single dimension with the expanded dimensions\n",
        "            expanded_dims = [kwargs[dim] for dim in components]\n",
        "            new_shape.extend(expanded_dims)\n",
        "        else:\n",
        "            # Add the size of the dimension directly from the input shape\n",
        "            new_shape.append(original_shape[index])\n",
        "\n",
        "    # Reshape the array\n",
        "    transformed_array = array.reshape(*new_shape)\n",
        "    return transformed_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbLW5l4jAmDj"
      },
      "source": [
        "Update token mapping based on input transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gASNqz2PAlak"
      },
      "outputs": [],
      "source": [
        "def update_input_tokens_mapping(input_tokens_mapping, **kwargs):\n",
        "    \"\"\"\n",
        "    Expands the input tokens mapping based on additional arguments, while preserving ellipsis (`...`) position.\n",
        "\n",
        "    Parameters:\n",
        "    - input_tokens_mapping: dict, maps tokens to indices in the array.\n",
        "    - kwargs: dict, additional arguments for expanding dimensions (e.g., h, w, etc.).\n",
        "\n",
        "    Returns:\n",
        "    - dict: Expanded token mapping with proper indices for all tokens, including ellipsis.\n",
        "    \"\"\"\n",
        "    expanded_tokens_mapping = {}\n",
        "    current_index = 0  # Track the current index in the mapping\n",
        "\n",
        "    for token, index in input_tokens_mapping.items():\n",
        "        if token == '...':  # Handle ellipsis\n",
        "            if index == []:  # If ellipsis is empty, keep it as is\n",
        "                expanded_tokens_mapping[token] = index\n",
        "            else:\n",
        "                # Preserve the ellipsis with its original indices\n",
        "                expanded_tokens_mapping[token] = index\n",
        "                current_index = max(index) + 1 if index else current_index\n",
        "        elif '(' in token and ')' in token:  # Handle grouped tokens like '(h w)'\n",
        "            # Extract components inside parentheses\n",
        "            components = token.strip('()').split()\n",
        "\n",
        "            # Ensure all components are present in kwargs\n",
        "            if not all(dim in kwargs for dim in components):\n",
        "                raise ValueError(f\"Missing dimensions {components} for expanding '{token}'.\")\n",
        "\n",
        "            # Expand the components and assign sequential indices\n",
        "            for dim in components:\n",
        "                expanded_tokens_mapping[dim] = current_index\n",
        "                current_index += 1\n",
        "        else:\n",
        "            # Assign sequential indices for direct tokens\n",
        "            expanded_tokens_mapping[token] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "    return expanded_tokens_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKRLMRSQwqQE"
      },
      "source": [
        "### Output Pattern-based Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEP-YiLqAvJI"
      },
      "source": [
        "Class defined for all output pattern related transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H98NeVxoAtUk"
      },
      "outputs": [],
      "source": [
        "from math import prod\n",
        "\n",
        "class Output_Transformations:\n",
        "    def __init__(self, array, token_mapping, output_mapping):\n",
        "        self.array = array\n",
        "        self.token_mapping = token_mapping\n",
        "        self.output_mapping = output_mapping\n",
        "\n",
        "        self.output_order = list(self.output_mapping.keys())\n",
        "        self.input_order = list(self.token_mapping.keys())\n",
        "\n",
        "        singleton_values = [\n",
        "                int(key.split(\"_\")[1])  # Extract and convert the number part to an integer\n",
        "                for key in list(self.token_mapping.keys())\n",
        "                if key.startswith(\"singleton_\")  # Check if the key starts with \"singleton_\"\n",
        "            ]\n",
        "\n",
        "        self.last_singleton_value = max(singleton_values, default=0)\n",
        "\n",
        "        self.last_index_val = max(\n",
        "                    max(val) if isinstance(val, list) and len(val) > 0 else val\n",
        "                    for val in self.token_mapping.values()\n",
        "                )\n",
        "\n",
        "        # print(\"Last Singleton Value: \", self.last_singleton_value)\n",
        "\n",
        "    def remove_singleton(self):\n",
        "        \"\"\"\n",
        "        If singleton is in input mapping, but is not needed in output mapping, then remove it from input mapping and array.\n",
        "        \"\"\"\n",
        "\n",
        "        remove_arr = []\n",
        "\n",
        "        for token in self.input_order:\n",
        "            if \"singleton\" in token:\n",
        "                if token not in self.stripped_order(self.output_order):\n",
        "                    remove_arr.append(self.token_mapping[token])\n",
        "                    del self.token_mapping[token]\n",
        "\n",
        "        if len(remove_arr) > 0:\n",
        "            # print(\"Removing singletons\")\n",
        "            # print(remove_arr)\n",
        "            for count, key in enumerate(self.token_mapping.keys()):\n",
        "                self.token_mapping[key] = count\n",
        "            # print(self.token_mapping)\n",
        "            self.array = self.array.squeeze(axis=tuple(remove_arr))\n",
        "\n",
        "    def stripped_order(self, d):\n",
        "        return \" \".join(d).replace(\"(\", \"\").replace(\")\", \"\").split(\" \")\n",
        "\n",
        "    def add_singleton(self):\n",
        "        \"\"\"\n",
        "        If singleton is in output mapping, but is not in input mapping, then add it to output array shape.\n",
        "        \"\"\"\n",
        "\n",
        "        count = 0\n",
        "        for token in self.stripped_order(self.output_order):\n",
        "            if \"singleton\" in token:\n",
        "                if token not in self.stripped_order(self.input_order):\n",
        "                    count += 1\n",
        "                    self.token_mapping[\"singleton_\"+str(self.last_singleton_value+count)] = self.last_index_val + count\n",
        "\n",
        "        new_shape = list(self.array.shape)\n",
        "\n",
        "        for i in range(count):\n",
        "            new_shape.append(1)\n",
        "\n",
        "        self.array = self.array.reshape(*new_shape)\n",
        "\n",
        "    def reorder_array(self):\n",
        "        \"\"\"\n",
        "        Based on the order of output identifiers (after removing paranthesis), re-order the array.\n",
        "        \"\"\"\n",
        "\n",
        "        output_order_stripped = self.stripped_order(self.output_order)\n",
        "        # print(output_order_stripped)\n",
        "\n",
        "        reshape_order = []\n",
        "        for token in output_order_stripped:\n",
        "            if '...' in token:\n",
        "                reshape_order += self.token_mapping[token]\n",
        "            else:\n",
        "                reshape_order.append(self.token_mapping[token])\n",
        "\n",
        "        # print(reshape_order)\n",
        "        self.reshaped_array = self.array.transpose(*reshape_order)\n",
        "\n",
        "    def resum_array(self):\n",
        "        \"\"\"\n",
        "        Based on if paranthesis exist in output, reshape the array.\n",
        "        \"\"\"\n",
        "\n",
        "        s = self.array.shape\n",
        "\n",
        "        new_order = []\n",
        "        for token in self.output_order:\n",
        "            if \"(\" in token:\n",
        "                inner_tokens = token.strip('()').split()\n",
        "                # print(inner_tokens)\n",
        "                new_shape = prod([s[self.token_mapping[inner_t]] for inner_t in inner_tokens])\n",
        "                new_order.append(new_shape)\n",
        "            elif '...' in token:\n",
        "                new_order += [s[inner_t] for inner_t in self.token_mapping[token]]\n",
        "            else:\n",
        "                new_order.append(s[self.token_mapping[token]])\n",
        "        # print(new_order)\n",
        "        self.resummed_array = self.reshaped_array.reshape(*new_order)\n",
        "        # print(\"Array resummed: \", self.resummed_array.shape)\n",
        "\n",
        "    def _requires_reshaping(self):\n",
        "        \"\"\"\n",
        "        Checks to see if paranthesis exist in output.\n",
        "        \"\"\"\n",
        "\n",
        "        paranth_flag = False\n",
        "        for token in self.output_order:\n",
        "            if(\"(\" in token):\n",
        "                paranth_flag = True\n",
        "\n",
        "        return paranth_flag\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "        1. Calls remove singleton function\n",
        "        2. Calls add singleton function\n",
        "        3. Calls reorder array function\n",
        "        4. Calls resum array function is reshaping is required\n",
        "        \"\"\"\n",
        "\n",
        "        self.remove_singleton()\n",
        "        self.add_singleton()\n",
        "\n",
        "        # print(self.array.shape)\n",
        "\n",
        "        self.reorder_array()\n",
        "\n",
        "        # print(\"Array reshaped: \", self.reshaped_array.shape)\n",
        "\n",
        "        if self._requires_reshaping():\n",
        "            self.resum_array()   # Step 4: Reshape grouped dimensions\n",
        "            return self.resummed_array\n",
        "        else:\n",
        "            return self.reshaped_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSW_Pe95A2do"
      },
      "source": [
        "Final function is ready to be defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA8LMb-vAytV"
      },
      "outputs": [],
      "source": [
        "def rearrange(array, pattern, **kwargs):\n",
        "    \"\"\"\n",
        "    Rearranges an array based on the einops-like pattern and additional arguments.\n",
        "\n",
        "    Steps:\n",
        "    1. Validate the input array and pattern.\n",
        "    2. Process extra arguments and parentheses.\n",
        "    3. Perform input-based transformations.\n",
        "    4. Update input tokens mapping based on transformations.\n",
        "    5. Apply output transformations.\n",
        "    6. Return the final transformed array.\n",
        "    \"\"\"\n",
        "\n",
        "    v = Validator(array, pattern, **kwargs)\n",
        "\n",
        "    array, input_tokens_mapping, input_tokens_shape_mapping, output_tokens_mapping = v.validate_and_return()\n",
        "\n",
        "    check_extra_arguments(input_tokens_mapping, **kwargs)\n",
        "    all_args = get_additional_args(input_tokens_mapping, input_tokens_shape_mapping, **kwargs)\n",
        "\n",
        "    transformed_array = input_based_transformation(\n",
        "                                                array,\n",
        "                                                input_tokens_mapping,\n",
        "                                                input_tokens_shape_mapping,\n",
        "                                                **all_args)\n",
        "\n",
        "    updated_tokens_mapping = update_input_tokens_mapping(\n",
        "        input_tokens_mapping,\n",
        "        **all_args,\n",
        "    )\n",
        "\n",
        "    d = Output_Transformations(transformed_array, token_mapping=updated_tokens_mapping, output_mapping=output_tokens_mapping)\n",
        "    return d.transform()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrNAxGl6mD_3"
      },
      "source": [
        "### Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIxcnNDQX1sS"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestValidator(unittest.TestCase):\n",
        "    def test_valid_pattern(self):\n",
        "        array = np.ones((2, 3, 4))\n",
        "        pattern = \"a b c -> b a c\"\n",
        "        v = Validator(array, pattern)\n",
        "        self.assertEqual(v.input_tokens, [\"a\", \"b\", \"c\"])\n",
        "        self.assertEqual(v.output_tokens, [\"b\", \"a\", \"c\"])\n",
        "\n",
        "    def test_ellipsis_handling(self):\n",
        "        array = np.ones((2, 3, 4, 5))\n",
        "        pattern = \"... c -> c ...\"\n",
        "        v = Validator(array, pattern)\n",
        "        v.validate_and_return()\n",
        "        self.assertIn(\"...\", v.input_tokens_mapping)\n",
        "        self.assertEqual(len(v.input_tokens_mapping[\"...\"]), 3)\n",
        "\n",
        "    def test_invalid_pattern(self):\n",
        "        array = np.ones((2, 3))\n",
        "        pattern = \"a b -> a b c\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            Validator(array, pattern).validate_and_return()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOnpOeTDZEJY"
      },
      "outputs": [],
      "source": [
        "class TestRearrange(unittest.TestCase):\n",
        "    def test_basic_rearrange(self):\n",
        "        array = np.arange(6).reshape(2, 3)  # Shape: (2, 3)\n",
        "        pattern = \"a b -> b a\"\n",
        "        result = rearrange(array, pattern)\n",
        "        expected = np.array([[0, 3], [1, 4], [2, 5]])  # Transposed\n",
        "        np.testing.assert_array_equal(result, expected)\n",
        "\n",
        "    def test_singleton_dimension(self):\n",
        "        array = np.random.randn(2, 1, 3)  # Shape: (2, 1, 3)\n",
        "        pattern = \"a 1 b -> b a\"\n",
        "        result = rearrange(array, pattern)\n",
        "        expected = np.random.randn(3, 2)  # Singleton dimension removed\n",
        "        np.testing.assert_array_equal(result.shape, expected.shape)\n",
        "\n",
        "    def test_ellipsis(self):\n",
        "        array = np.ones((2, 3, 4, 5))  # Shape: (2, 3, 4, 5)\n",
        "        pattern = \"... c -> c ...\"\n",
        "        result = rearrange(array, pattern)\n",
        "        expected = np.ones((5, 2, 3, 4))  # Move the last dimension to the front\n",
        "        np.testing.assert_array_equal(result, expected)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIuvJUPMeckc"
      },
      "outputs": [],
      "source": [
        "class TestOutputTransformations(unittest.TestCase):\n",
        "    def test_remove_singleton(self):\n",
        "        array = np.ones((2, 1, 3))  # Shape: (2, 1, 3)\n",
        "        token_mapping = {\"a\": 0, \"singleton_1\": 1, \"b\": 2}\n",
        "        output_mapping = {\"b\": 0, \"a\": 1}\n",
        "        d = Output_Transformations(array, token_mapping, output_mapping)\n",
        "        d.remove_singleton()\n",
        "        self.assertEqual(d.array.shape, (2, 3))\n",
        "        self.assertNotIn(\"singleton_1\", d.token_mapping)\n",
        "\n",
        "    def test_add_singleton(self):\n",
        "        array = np.ones((2, 3))  # Shape: (2, 3)\n",
        "        token_mapping = {\"a\": 0, \"b\": 1}\n",
        "        output_mapping = {\"singleton_1\": 0, \"a\": 1, \"b\": 2}\n",
        "        d = Output_Transformations(array, token_mapping, output_mapping)\n",
        "        o = d.transform()\n",
        "        self.assertEqual(o.shape, (1, 2, 3))  # Singleton added\n",
        "        self.assertIn(\"singleton_1\", d.token_mapping)\n",
        "\n",
        "    def test_reshape_array(self):\n",
        "        array = np.ones((2, 3, 4))  # Shape: (2, 3, 4)\n",
        "        token_mapping = {\"a\": 0, \"b\": 1, \"c\": 2}\n",
        "        output_mapping = {\"c\": 0, \"a\": 1, \"b\": 2}\n",
        "        d = Output_Transformations(array, token_mapping, output_mapping)\n",
        "        d.reorder_array()\n",
        "        self.assertEqual(d.reshaped_array.shape, (4, 2, 3))\n",
        "\n",
        "class TestRearrangeFunctions(unittest.TestCase):\n",
        "    def test_varied_cases(self):\n",
        "        all_tests = [{'array_shape': (2, 3, 12, 6),\n",
        "        'pattern': 'b h (h1 h2) c -> b c h1 h2 h',\n",
        "        'args': {'h1': 4},\n",
        "        'einops_ground_truth': (2, 6, 4, 3, 3)},\n",
        "        {'array_shape': (2, 3, 12, 6),\n",
        "        'pattern': 'b h w c -> b w h c',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 12, 3, 6)},\n",
        "        {'array_shape': (2, 3, 12, 6),\n",
        "        'pattern': 'b h w c -> (b h) w c',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (6, 12, 6)},\n",
        "        {'array_shape': (2, 3, 12, 6),\n",
        "        'pattern': 'b h w c -> b (c h w)',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 216)},\n",
        "        {'array_shape': (2, 12, 18, 6),\n",
        "        'pattern': 'b (h1 h) (w1 w) c -> (b h1 w1) h w c',\n",
        "        'args': {'h1': 3, 'h': 4, 'w1': 3, 'w': 6},\n",
        "        'einops_ground_truth': (18, 4, 6, 6)},\n",
        "        {'array_shape': (2, 12, 18, 6),\n",
        "        'pattern': 'b (h h1) (w w1) c -> b h w (c h1 w1)',\n",
        "        'args': {'h1': 3, 'w': 6},\n",
        "        'einops_ground_truth': (2, 4, 6, 54)},\n",
        "        {'array_shape': (2, 12, 18, 6),\n",
        "        'pattern': '... h w -> ... (h w)',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 12, 108)},\n",
        "        {'array_shape': (2, 12, 18, 6),\n",
        "        'pattern': '... (h w) c -> ... (h w c)',\n",
        "        'args': {'w': 6},\n",
        "        'einops_ground_truth': (2, 12, 108)},\n",
        "        {'array_shape': (2, 3),\n",
        "        'pattern': '... h w -> ... w h 1',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (3, 2, 1)},\n",
        "        {'array_shape': (2, 3, 4),\n",
        "        'pattern': 'b c h -> b h c',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 4, 3)},\n",
        "        {'array_shape': (2, 3, 4),\n",
        "        'pattern': 'b c h -> b (c h) 1',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 12, 1)},\n",
        "        {'array_shape': (2, 3, 4),\n",
        "        'pattern': 'b c h -> b h c 1 1',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 4, 3, 1, 1)},\n",
        "        {'array_shape': (2, 3, 4, 1),\n",
        "        'pattern': 'b c h 1 -> b c h',\n",
        "        'args': {},\n",
        "        'einops_ground_truth': (2, 3, 4)}\n",
        "        ]\n",
        "\n",
        "        for test in all_tests:\n",
        "            array = np.random.randn(*test['array_shape'])\n",
        "            result = rearrange(array, test['pattern'], **test['args'])\n",
        "            self.assertEqual(result.shape, test['einops_ground_truth'])\n",
        "\n",
        "    def test_incompatible_patterns(self):\n",
        "        array = np.random.randn(2, 3, 4)\n",
        "        patterns = [\n",
        "            'a b c -> a c',\n",
        "            'a b 1 -> a b',\n",
        "            'a (b c) -> a b c',\n",
        "            'a b (c1 c2) -> a b c1 c2'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            with self.assertRaises(ValueError):\n",
        "                rearrange(array, pattern)\n",
        "\n",
        "    def test_incompatible_arguments(self):\n",
        "        array = np.random.randn(32, 30, 120)\n",
        "        pattern = 'b h (w1 w2) -> w1 h b w2'\n",
        "        args = {'w1': 11}\n",
        "\n",
        "        with self.assertRaises(ValueError):\n",
        "            rearrange(array, pattern, **args)\n",
        "\n",
        "        array = np.random.randn(32, 30, 120)\n",
        "        pattern = 'b h (w1 w2 w3) -> w1 h b w2 w3'\n",
        "        args = {'w1': 12}\n",
        "\n",
        "        with self.assertRaises(ValueError):\n",
        "            rearrange(array, pattern, **args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBfWimO2ek5F",
        "outputId": "2b0372fa-2cd4-4e70-e45d-a75ee06004cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "test_add_singleton (__main__.TestOutputTransformations) ... ok\n",
            "test_remove_singleton (__main__.TestOutputTransformations) ... ok\n",
            "test_reshape_array (__main__.TestOutputTransformations) ... ok\n",
            "test_basic_rearrange (__main__.TestRearrange) ... ok\n",
            "test_ellipsis (__main__.TestRearrange) ... ok\n",
            "test_singleton_dimension (__main__.TestRearrange) ... ok\n",
            "test_incompatible_arguments (__main__.TestRearrangeFunctions) ... ok\n",
            "test_incompatible_patterns (__main__.TestRearrangeFunctions) ... ok\n",
            "test_varied_cases (__main__.TestRearrangeFunctions) ... ok\n",
            "test_ellipsis_handling (__main__.TestValidator) ... ok\n",
            "test_invalid_pattern (__main__.TestValidator) ... ok\n",
            "test_valid_pattern (__main__.TestValidator) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 12 tests in 0.126s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x79077739fdc0>"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcK26QK_v6tR"
      },
      "source": [
        "### Examples of Using Custom Rearrange Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vse9aZNv2Nj",
        "outputId": "8216e014-67e7-49f0-ba2e-79ed83728880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 3)\n"
          ]
        }
      ],
      "source": [
        "# Transpose\n",
        "x = np.random.rand(3, 4)\n",
        "result = rearrange(x, 'h w -> w h')\n",
        "print(result.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg69RqhUwCgy",
        "outputId": "f45ce578-3543-41ae-d4c8-f7dab2ae5d33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 4, 10)"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Split an axis\n",
        "x = np.random.rand(12, 10)\n",
        "result = rearrange(x, '(h w) c -> h w c', h=3)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcU18fWhwHJA",
        "outputId": "0c586fc1-49e9-4b84-a2ea-6206a78283fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12, 5)"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Merge axes\n",
        "x = np.random.rand(3, 4, 5)\n",
        "result = rearrange(x, 'a b c -> (a b) c')\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qaz_r94OwI5h",
        "outputId": "b5efaf5b-bc1e-434c-8b48-0e5c867ba4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 1, 1, 5)"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Repeat an axis\n",
        "x = np.random.rand(3, 1, 5)\n",
        "result = rearrange(x, 'a b c -> a b 1 c')\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N71uZwkAwKYZ",
        "outputId": "4d55c30d-2d77-4745-c9dc-b679f907c1b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2, 3, 20)"
            ]
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Handle batch dimensions\n",
        "x = np.random.rand(2, 3, 4, 5)\n",
        "result = rearrange(x, '... h w -> ... (h w)')\n",
        "result.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
